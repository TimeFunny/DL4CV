[TOC]

# Training 

## 1. tricks
1. ImageNet Training in Minutes(2017.09)
2. Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour(2017)
3. **Bag of Tricks for Image Classification with Convolutional Neural Networks(2018.12)**
4. **Bag of Freebies for Training Object Detection Neural Networks(2019.02)**

### Semi-supervised

1. Self-training with Noisy Student improves ImageNet classification(2019.11)

### Unsupervised

1. **MoCo**: Momentum Contrast for Unsupervised Visual Representation Learning(2019.11)


## 2. Learning rate decay
1. SGDR: Stochastic Gradient Descent with Warm Restarts(cosine schedule)
2. [Blog](https://zhuanlan.zhihu.com/p/32923584)
3. [GluonCV](https://zhuanlan.zhihu.com/p/38509951)


## 3. Activation/loss function
1. [activation](https://en.wikipedia.org/wiki/Activation_function)
2. Adaptive Sampled Softmax with Kernel Based Sampling(2018)

## 4. Regularization
1. ShakeDrop regularization(2018)
2. Shakeout: A New Regularized Deep Neural Network Training Scheme(2016)
3. Dropout: A Simple Way to Prevent Neural Networks from overfitting    


## 5. Normalization
1. Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift
2. Instance Normalization: The Missing Ingredient for Fast Stylization(2016.07)
3. Layer Normalization(2016.07)
4. Group Normalization(2018.03)
4. Differentiable Learning-to-Normalize via Switchable Normalization(2018.06)